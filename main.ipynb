{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3 - What movie to watch tonight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser_utils import clean_ls\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import utils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = json.load(open('urls.json'))\n",
    "voc = json.load(open('vocabulary.json'))\n",
    "inverted_index = json.load(open('inverted_index.json'))\n",
    "inverted_index_freq = json.load(open('inverted_index_freq.json'))\n",
    "actors_dict = json.load(open('actors_dict.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection\n",
    "\n",
    "## 1.1) Get the list of movies\n",
    "\n",
    "The first thing we had to do was to build our own dataset of movies. Each of us downloaded all the Wikipedia webpages indicated in one of the documents *movies1.html, movies2.html* and *movies3.html* that we were provided. We created to following class to parse the *html* pages and get the complete list of *urls*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag != 'a':\n",
    "            return\n",
    "        self.urls.append(attrs[0][1])\n",
    "\n",
    "page = open('movies1.html').read()  # page = open('movies2.html').read() ; page = open('movies3.html').read()\n",
    "parser = MyHTMLParser()\n",
    "parser.urls = []\n",
    "parser.feed(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved the list of *urls* in a *.json* file to easily access them is different steps of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for n, url in enumerate(parser.urls):\n",
    "    d[n]=url\n",
    "\n",
    "with open('urls.json', 'w') as fp:\n",
    "    json.dump(d, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Crawl Wikipedia\n",
    "\n",
    "Once we we got all the *urls*, we downloaded the corresponding Wikipedia *html* webpages. Each of us did this step separetly, therefore we iterated over different ranges of indexes so that at the end we could join all the documents witout overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):   # range(10000,20000); range(20000,30000)\n",
    "    try:\n",
    "        download_page(parser.urls[i])\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        time.sleep(1200)  # this exception happens when we reach the limit of requests of Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above makes use of the function *download_page* which save the *html* page of a given *url* and which is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(url):\n",
    "    req = urllib.request.urlopen(url)\n",
    "    webpage = str(req.read())\n",
    "    with open(\"article_%d.html\" %i, \"w\") as file:\n",
    "        file.write(webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Parse downloaded pages\n",
    "\n",
    "In this step of the homework we parsed the downloaded pages to extract information from each of them and save the results in separate *.tsv* files structured as requested. In particular, we have selected these data:\n",
    "1. **Title**: the text contained in the tag *title*  \n",
    "2. **Intro**: the text contained in the first *p* tag\n",
    "3. **Plot**: the text contained in the second *p* tag, when existing, otherwise *'NA'*\n",
    "4. to 14. **Infobox**: when present in the parsed page a *table* tag of class *infobox vevent*, we extract the following information (if provided, otherwise *'NA'*): film_name, director, producer, writer, starring, music, release date, runtime, country, language, budget.\n",
    "\n",
    "We used the librery **Beautiful Soup** to carry out the parsing process. For each *html* page, we first made a \"soup\" obtained by converting the document to Unicode, then we passed this soup our *parse_page* function and finally we save the result in a *.tsv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_wiki = ['Directed by', 'Produced by', 'Written by', 'Starring', 'Music by', 'Release date', 'Running time', 'Country', 'Language', 'Budget']\n",
    "head = ['title', 'intro', 'plot', 'film_name', 'director', 'producer', 'writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget']\n",
    "infos = ['NA']*14\n",
    "\n",
    "for i in range(10000):  # range(10000,20000); range(20000,30000)\n",
    "    page = open('webpages/articles/article_%d.html' %i, encoding = 'utf-8').read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    infos = parse_page(soup)\n",
    "    \n",
    "    with open('output_%d.tsv'%i, 'w', encoding = 'utf-8') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(head)\n",
    "        tsv_writer.writerow(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(soup):\n",
    "    infos_local = infos\n",
    "    infos_local[:2] = [soup.title.text, soup.find_all('p')[0].text]\n",
    "    if bool(soup.findAll('table', {'class':'infobox vevent'})) == False:\n",
    "        if len(soup.find_all('p'))>1:\n",
    "            infos_local[2] = [soup.find_all('p')[1].text]\n",
    "    else:\n",
    "        infobox = soup.find_all('table', {'class':'infobox vevent'})[0].findAll('tr')\n",
    "        if len(soup.find_all('p'))>1:\n",
    "            infos_local[2] = [soup.find_all('p')[1].text]\n",
    "        infos_local[3] =[infobox[0].text]\n",
    "        \n",
    "        # complete the list 'infos' with the available information\n",
    "        for j in range(len(head_wiki)):\n",
    "            for i in range(1,len(infobox)):\n",
    "                if bool(infobox[i].find('th')):\n",
    "                    if head_wiki[j] == infobox[i].th.text:\n",
    "                        infos_local[j+4] = infobox[i].td.get_text(separator=', ')   # names in head_wiki correspond to those in head[4:]\n",
    "                        break\n",
    "    return infos_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search Engine\n",
    "\n",
    "Before starting building the search engines we uesd the library **ntlk (Natural Language Toolkit)** to processes all the *.tsv* files and generate their simplified version. This step was of fundamental importance for the realization of a global vocabulary of all the (meaningful) words contained in the webpages.\n",
    "\n",
    "During this process, we removed both stopwords and punctuation; we also reduced each word to its root form. This is the function we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ls(list_strings):\n",
    "    clean = []\n",
    "    for string in list_strings:\n",
    "        words = string.split()\n",
    "        clean_string = ''\n",
    "        for term in words:\n",
    "            if not term in set(stopwords.words('english')) and term.isalnum():\n",
    "                clean_string += ' '+ps.stem(term)\n",
    "        if clean_string.strip() != '':\n",
    "            clean.append(clean_string.strip())\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we applied this function to all our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):    # range(10000,20000); range(20000,30000)\n",
    "    file = open('tsv/output_%d.tsv' %i, encoding = 'ISO-8859-1') \n",
    "    lines = file.read().split('\\n\\n') \n",
    "    tabs = lines[1].split('\\t')   # we only process the part of the tsv files with information about the movies (not the headers)\n",
    "    with open('filtered_%d.tsv'%i,'a', encoding = 'utf-8')  as f:\n",
    "        tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "        tsv_writer.writerow(head) \n",
    "        tsv_writer.writerow(clean_ls(tabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1) Conjunctive query\n",
    "\n",
    "Before building the indexes of our search engines, we joined the documents of the 30000 movies and create a vocabulary of the words in them, that is, a file in which each *term* is associated to an integer (*term_id*). We only focused on the words contained in the *intro* and *plot* of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set() \n",
    "for i in range(30000):\n",
    "    file = open('webpages/tsv clean/filtered_%d.tsv'%i).read().split('\\n\\n')[1]\n",
    "    tabs = file.split('\\t')[1]+file.split('\\t')[2]  # list of words in intro and plots\n",
    "    words.update(tabs.split())\n",
    "\n",
    "voc = dict()\n",
    "for term_id, term in enumerate(words):\n",
    "    voc[term]= term_id  \n",
    "    \n",
    "with open('vocabulary.json', 'w') as fp:\n",
    "    json.dump(voc, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of which search engine we decide to use to return the result of the query, there are few steps that must be executed and that are in common for all of them.\n",
    "\n",
    "First we ask the user to enter a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('What do you want to search?')\n",
    "query = input().split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we \"clean\" the input in order to reduce it to the same format of the words in our vocabulary. For this aim we use the same functino *clean_ls* previously used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = clean_ls(query)   # list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we extract from the vocabulary the *term_id*s of the words in the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = [voc[word] for word in query]   # integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1)  Create your index!\n",
    "\n",
    "In this step we have created the inverted index of the first search engine and stored it in a separate file. The inverted index is a vocabulary having the *term_id'*s of the words as keys and a list of the *documents_id'*s in which the corresponding term appeared as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = dict.fromkeys(range(len(voc)), [])\n",
    "for i in range(30000):     \n",
    "    file = open('webpages/tsv clean/filtered_%d.tsv'%i).read()\n",
    "    lines = file.split('\\n\\n')[1]\n",
    "    tabs = lines.split('\\t')[1]+lines.split('\\t')[2]  # list of words in intro and plots\n",
    "    for word in set(tabs.split()):\n",
    "        inverted_index[voc[word]] = inverted_index[voc[word]]+[i]\n",
    "\n",
    "with open('inverted_index.json', 'w') as fp:\n",
    "    json.dump(inverted_index, fp)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the inverted index we can already select all the movies that match the query entered by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDOC = [inverted_index[str(word)] for word in query_index]\n",
    "query_match = set(allDOC[0]).intersection(*allDOC[1:])   # list of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query\n",
    "\n",
    "The function ***search_engine_3*** we wrote for the first search engine takes as parameter the list of the identifiers of the movies matching the query entered by the user (already processed as previously explained) and returns a dataframe listing all of them. For each movie, the dataframe shows title, intro and the wikipedia url. We also used an auxiliary function *make_clickable* to transform the urls in links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_1(query_match):\n",
    "    df = pd.DataFrame(columns=['Title','Intro','Wikipedia Url'])\n",
    "    for i in query_match:\n",
    "        file = open('webpages/tsv/output_%d.tsv' %i).read().split('\\n\\n')[1].split('\\t')\n",
    "        title, intro, link = file[3].encode('utf8').decode(\"unicode_escape\"), file[1].encode('utf8').decode(\"unicode_escape\"), urls[str(i)]\n",
    "        new_row = {'Title':title, 'Intro': intro, 'Wikipedia Url': link}\n",
    "        df = df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    d = dict(selector=\"th\", props=[('text-align', 'center')])\n",
    "    df.style.format({'Wikipedia Url': utils.make_clickable}).hide_index().set_table_styles([d]).set_properties(**{'text-align': 'center'}).set_properties(subset=['Title'], **{'width': '130px'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Conjunctive query & Ranking score\n",
    "\n",
    "In this part of the homework we wrote a function for a more refined search engine whose peculiarity is to rank the result of the query by their **TF-IDF** (Term Frequency - Inverse Document Frequency) score and thus it returns a dataframe showing the top 5 movies with the highest TF-IDF.\n",
    "\n",
    "In order to build the new inverted index in which the TF-IDF scores are also included, we first had to implement some functions to calculate it.\n",
    "\n",
    "The **TD-IDF** is a function of both the *term_id* and the *document_id* to which the specific term belongs to. It is defined as the product of two values: the **data frequency** of the term in the selected document and the **inverse data frequency** of the term in the whole set of documents. It can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the data frequency (integer) of a term (string) in a specified document (i)\n",
    "def data_freq(term, i):\n",
    "    file = open('webpages/tsv clean/filtered_%d.tsv'%i).read().split('\\n\\n')[1]\n",
    "    tabs = file.split('\\t')[1]+file.split('\\t')[2]  # list of words in intro and plots\n",
    "    if term in tabs.split():\n",
    "        df = tabs.split().count(term)/len(tabs.split())\n",
    "    else:\n",
    "        df = 0\n",
    "    return df\n",
    "\n",
    "# Function that returns the inverse data frequency of a word in the vocabulary \n",
    "def idf(term):\n",
    "    val = len(inverted_index[str(voc[term])])  # this is the number of documents containing the given word\n",
    "    return math.log(N/val)\n",
    "\n",
    "\n",
    "# Function that returns the term frequency - inverse document frequency \n",
    "def tfidf(term_id, document_id): # (integer, integer)\n",
    "    return data_freq(get_key(term_id), document_id)*idf(get_key(term_id))\n",
    "\n",
    "# Function that returns the a term given its term_id\n",
    "def get_key(term_id):  \n",
    "    for key, value in voc.items(): \n",
    "         if term_id == value:\n",
    "                return key   \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we also used the auxiliary function *get_key* to extract a key from the vocabulary given a specific values (keys and values are in a bijective relation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index\n",
    "\n",
    "At this point the new inverted index *inverted_index_freq* can be built. The difference with the previous one is that now in the list of *document_id'* s corresponding to each *term_id* the TF-IDF scores are also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_freq = dict.fromkeys(range(len(voc)))\n",
    "for term_id in inverted_index_freq.keys():\n",
    "    for document_id in inverted_index[str(term_id)]:\n",
    "        if inverted_index_freq[term_id] == None:\n",
    "            inverted_index_freq[term_id] = [(document_id, tfidf(term_id, document_id))]\n",
    "        else:\n",
    "            inverted_index_freq[term_id] += [(document_id, tfidf(term_id, document_id))]\n",
    "\n",
    "# The new inverted_index_freq is as follow: {term_id : [document_id1, TF-IDF_{document_id1, term}], [document_id2, TF-IDF_{document_id2, term}], ...] \n",
    "\n",
    "with open('inverted_index_freq.json', 'w') as fp:\n",
    "    json.dump(inverted_index_freq, fp)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query\n",
    "\n",
    "We define the function ***search_engine_2*** that returns the top 5 movies having the highest similarity with the query. To mantain the *top-5* documents we use the heap structure provided by the library **heapq**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_2(query, query_match):\n",
    "    #Build the query vector\n",
    "    query_vec = utils.query_vector(query)\n",
    "\n",
    "    # Build the heap structure\n",
    "    sim_dict = {}\n",
    "    for i in query_match:\n",
    "        sim_dict[i] = utils.cosine_similarity(query_vec, i)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Title','Intro','Wikipedia Url', 'Similarity'])\n",
    "\n",
    "    for sim in heapq.nlargest(5, sim_dict.items(), key = lambda i: i[1]):\n",
    "        i = sim[0]  # document_id\n",
    "        file = open('webpages/tsv/output_%d.tsv' %i).read().split('\\n\\n')[1].split('\\t')\n",
    "        title, intro, link = file[3].encode('utf8').decode(\"unicode_escape\"), file[1].encode('utf8').decode(\"unicode_escape\"), urls[str(i)]\n",
    "        new_row = {'Title':title, 'Intro': intro, 'Wikipedia Url': link, 'Similarity': sim[1]}\n",
    "        df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "    # Visualization of the top 5 documents related to the query\n",
    "    d = dict(selector=\"th\", props=[('text-align', 'center')])\n",
    "    df1 = df.sort_values(by=['Similarity'], ascending = False)\n",
    "    df1.style.format({'Wikipedia Url': utils.make_clickable}).hide_index().set_table_styles([d]).set_properties(**{'text-align': 'center'}).set_properties(subset=['Title'], **{'width': '130px'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above some more functions have been used to evaluate the similarity between the query and the documents that match it. In general, we imagine to represent each document as a vector in a high-dimensional vector space whose axis are given by the words in the vocabulary. In this representation, the value of a vector in each coordinate is given by the TD-IDF of that word in that specific document, in partiular it will be zero for all the axis corresponding to the words that do not appear in the document. Given this, the algorithm proceeds as follows:\n",
    "1. The query is transformed in a vector with same representation as describrd above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector(query): # list of strings\n",
    "    query_vector = {}\n",
    "    for word in query:  # words are strings\n",
    "        df = query.count(word)/len(query)\n",
    "        query_vector[voc[word]] = df * idf(word)\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The movies that match the query are loaded in their vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(i):  # integer\n",
    "    vec = {}\n",
    "    file = open('webpages/tsv clean/filtered_%d.tsv'%i).read().split('\\n\\n')[1]\n",
    "    tabs = file.split('\\t')[1]+file.split('\\t')[2]  # list of words in intro and plots\n",
    "    for word in tabs.split():\n",
    "        vec[voc[word]] = tfidf(voc[word], i)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The similarity between the query and each document is evaluated just calculating the cosine among the respective vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_vec, document_id):  # (dict, integer)\n",
    "    norm_query = math.sqrt(sum(n**2 for n in query_vec.values()))\n",
    "    norm_doc = math.sqrt(sum(tfidf(word,document_id) for word in vector(document_id)))\n",
    "    dot_pr = 0\n",
    "    for word in query_vec.keys():\n",
    "        dot_pr += query_vec[word]*tfidf(word, document_id)  # (string, integer)\n",
    "    return dot_pr/(norm_query*norm_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define a new score!\n",
    "\n",
    "In this part of the homework we define a new similarity based on the year of release of the movies matching the query. Specifically, we ask the user to enter the year of release of the movie he is looking for and we show him the 5 movies among the ones already selected whose year of release is closer to the one he requested.\n",
    "\n",
    "We define two more functions, the first (*years_docs*) to extract the year of release from the *.tsv* files, the second (*sim_docs*) to calculate the similiarity of each document in the match. If the year of release is not provided, then its similarity is simply set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_docs(set_docs):\n",
    "    years = {}\n",
    "    for i in set_docs:\n",
    "        file = open('webpages/tsv clean/filtered_%d.tsv' %i).read().split('\\n\\n')[1].split('\\t')\n",
    "        y = file[9]\n",
    "        if len(y) == 4:\n",
    "            years[i] = y\n",
    "        else:\n",
    "            res = re.search(r'\\d{4}', y)\n",
    "            if res != None:\n",
    "                years[i] = res.group(0)\n",
    "            else: \n",
    "                years[i] = 'NA'\n",
    "    return years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_docs(years, year_user):\n",
    "    l = []\n",
    "    for val in years.values():\n",
    "        if val != 'NA':\n",
    "            l.append(int(val))\n",
    "    N = max(abs(val-year_user) for val in l)\n",
    "    sim_years = {}\n",
    "    for key, values in years.items():\n",
    "        if values != 'NA':\n",
    "            sim_years[key] = 1-abs(int(values)-year_user)/N   # the similarity is normalized between 0 and 1\n",
    "        else:\n",
    "            sim_years[key] = 0\n",
    "    return sim_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these functions defined, we can build the ***search_engine_3***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_3(query_match):\n",
    "    print('In which year was the movie released?')\n",
    "    year_user = int(input())\n",
    "\n",
    "    # Rank the results by closeness to a given year\n",
    "    years = utils.year_docs(query_match)\n",
    "    sim_years = utils.sim_docs(years, year_user)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Title','Intro','Wikipedia Url', 'Similarity'])\n",
    "\n",
    "    for sim in heapq.nlargest(5, sim_years.items(), key = lambda i: i[1]):\n",
    "        i = sim[0]  # document_id\n",
    "        file = open('webpages/tsv/output_%d.tsv' %i).read().split('\\n\\n')[1].split('\\t')\n",
    "        title, intro, link = file[3].encode('utf8').decode(\"unicode_escape\"), file[1].encode('utf8').decode(\"unicode_escape\"), urls[str(i)]\n",
    "        new_row = {'Title':title, 'Intro': intro, 'Wikipedia Url': link, 'Similarity': sim[1]}\n",
    "        df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "    # Visualization of the top 5 documents related to the query\n",
    "    d = dict(selector=\"th\", props=[('text-align', 'center')])\n",
    "    df1 = df.sort_values(by=['Similarity'], ascending = False)\n",
    "    df1.style.format({'Wikipedia Url': utils.make_clickable}).hide_index().set_table_styles([d]).set_properties(**{'text-align': 'center'}).set_properties(subset=['Title'], **{'width': '130px'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Step: Make a nice visualization!\n",
    "\n",
    "The aim of this part of the homework is to make a visualization of the network representing the actors in the top 10 movies selected through the _search_engine_3_ who starred together in at least two of the movies lited in our database. \n",
    "\n",
    "In order to build faster the graph during the execution of the program, we have created and stored in the memory a dictionary in which each actor is associated to all the movies in which he starred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_dict = dict()\n",
    "for i in range(30000):\n",
    "    file = open('webpages/tsv/output_%d.tsv' %i, encoding = 'utf-8').read().split('\\n\\n')[1].split('\\t')\n",
    "    actors = file[7].split(', ')\n",
    "    for act in actors:\n",
    "        if act != '\\\\n' and act != 'NA' and act!= 'See below' and act != ' ':\n",
    "            if act not in actors_dict.keys():\n",
    "                actors_dict[act] = [i]\n",
    "            else:\n",
    "                actors_dict[act] += [i]\n",
    "                \n",
    "with open('actors_dict.json', 'w') as a:\n",
    "    json.dump(actors_dict, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the following functions to add nodes and edges to the graph and also to improve its visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the nodes to the graph\n",
    "def add_nodes(list_id):\n",
    "    G = nx.Graph()\n",
    "    for i in list_id:\n",
    "        file = open('webpages/tsv/output_%d.tsv' %i).read().split('\\n\\n')[1].split('\\t')\n",
    "        actors = file[7].split(', ')\n",
    "        for act in actors:\n",
    "            if act != '\\\\n':\n",
    "                G.add_node(act.strip()) \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the edges to the graph\n",
    "def add_edges(G):\n",
    "    comb = combinations(G.nodes, 2)\n",
    "    for couple in list(comb):\n",
    "        co_star = len(set(actors_dict[couple[0]]).intersection(actors_dict[couple[1]]))\n",
    "        if co_star >= 2:\n",
    "            G.add_edge(couple[0],couple[1], weight = co_star)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the network\n",
    "def draw_graph(G):\n",
    "    network = plt.figure(figsize=(15,8))\n",
    "    pos = nx.circular_layout(G)\n",
    "\n",
    "    degrees = G.degree() #D ict with Node ID, Degree\n",
    "    nodes = G.nodes()\n",
    "    n_color = np.asarray([degrees[n] for n in nodes])\n",
    "\n",
    "    d = dict(G.degree)\n",
    "    elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] > 4]\n",
    "    esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] <= 4]\n",
    "    nc = nx.draw_networkx_nodes(G, pos, node_size= [v * 100 for v in d.values()], node_color=n_color, cmap=plt.cm.jet, with_labels = False)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=elarge, edge_color='r', width = 5)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=esmall, edge_color='g', style='dashed')\n",
    "    plt.colorbar(nc).set_label('Number of movies in which the actor played')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can ask the user if he wants to see the *CO-STARDOM* network and, in case of positive answer, we can show him the graph generated by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CO-STARDOM NETWORK\n",
    "movies = [movie[0] for movie in heapq.nlargest(10, sim_years.items(), key = lambda i: i[1])]\n",
    "G = utils.add_nodes(movies)\n",
    "G = utils.add_edges(G)\n",
    "print(utils.draw_graph(G))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
